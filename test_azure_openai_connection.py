# -*- coding: utf-8 -*-
"""
ä»¥ LangChain çš„ AzureChatOpenAI è°ƒç”¨æ ¼å¼æµ‹è¯• Azure OpenAI è¿æ¥
"""
import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import AzureChatOpenAI

# -------------------------------------------------------------------
# 1) åŠ è½½ .envï¼ˆä¼˜å…ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
# -------------------------------------------------------------------
ROOT_DIR = Path(__file__).resolve().parent
env_path = ROOT_DIR / ".env"

if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒæ–‡ä»¶: {env_path}")
else:
    print("âš ï¸ æœªåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰¾åˆ° .envï¼Œå°†ç›´æ¥ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")

# -------------------------------------------------------------------
# 2) è¯»å–å¿…é¡»çš„ç¯å¢ƒå˜é‡
# -------------------------------------------------------------------
required_vars = [
    "AZURE_OPENAI_ENDPOINT",
    "AZURE_OPENAI_API_KEY",
    "AZURE_OPENAI_DEPLOYMENT",
    "AZURE_OPENAI_API_VERSION",
]

missing = [k for k in required_vars if not os.getenv(k)]
if missing:
    print("\nâŒ ç¼ºå°‘ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨ .env æˆ–ç³»ç»Ÿç¯å¢ƒä¸­è¡¥å…¨ï¼š")
    for k in missing:
        print(f"   - {k}")
    raise SystemExit(1)

endpoint    = os.environ["AZURE_OPENAI_ENDPOINT"]
api_key     = os.environ["AZURE_OPENAI_API_KEY"]
deployment  = os.environ["AZURE_OPENAI_DEPLOYMENT"]
api_version = os.environ["AZURE_OPENAI_API_VERSION"]

print("\nğŸ”§ å½“å‰é…ç½®ï¼š")
print(f"   AZURE_OPENAI_ENDPOINT   = {endpoint}")
print(f"   AZURE_OPENAI_DEPLOYMENT = {deployment}")
print(f"   AZURE_OPENAI_API_VERSION= {api_version}")

# -------------------------------------------------------------------
# 3) å¯é€‰ï¼šé…ç½®ä»£ç†ï¼ˆä»…å½“ .env / ç¯å¢ƒå˜é‡æä¾›æ—¶æ‰ç”Ÿæ•ˆï¼‰
#    æ”¯æŒ PROXY_URL / HTTPS_PROXY / HTTP_PROXY ä¸‰é€‰ä¸€ï¼›æ— éœ€å°±ä¸è®¾ç½®
# -------------------------------------------------------------------
proxy = os.getenv("PROXY_URL") or os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
no_proxy = os.getenv("NO_PROXY")
if proxy:
    os.environ["HTTP_PROXY"]  = proxy
    os.environ["HTTPS_PROXY"] = proxy
if no_proxy:
    os.environ["NO_PROXY"] = no_proxy
print(f"\nğŸŒ Proxy configured: {'YES' if proxy else 'NO'}")

# -------------------------------------------------------------------
# 4) ç”¨ AzureChatOpenAI åˆå§‹åŒ–ï¼Œå‘èµ·ä¸€æ¬¡ç®€å•è¯·æ±‚
# -------------------------------------------------------------------
print("\nğŸš€ æ­£åœ¨å°è¯•è¿æ¥ Azure OpenAI...")

llm = AzureChatOpenAI(
    deployment_name=deployment,          # âœ… ç”¨éƒ¨ç½²å
    api_key=api_key,
    azure_endpoint=endpoint,
    api_version=api_version,
    temperature=1.0,
)

try:
    # ä¸ä½ ä¹‹å‰â€œèƒ½è·‘â€çš„æ ¼å¼ä¸€è‡´ï¼šç›´æ¥ invoke
    response = llm.invoke("Say hello from Azure GPT-5.")
    print("\nâœ… è°ƒç”¨æˆåŠŸï¼è¿”å›å†…å®¹ç¤ºä¾‹ï¼š")
    print("-" * 60)
    print(response.content)
    print("-" * 60)

    # å…¼å®¹åœ°è¾“å‡ºä¸€äº›å¯é€‰è°ƒè¯•ä¿¡æ¯
    model_info = getattr(llm, "model_name", deployment)
    print("\nğŸ“Š è°ƒè¯•ä¿¡æ¯ï¼š")
    print(f"   model/deployment: {model_info}")
    print(f"   endpoint:         {endpoint}")
    print(f"   api_version:      {api_version}")

except Exception as e:
    print("\nâŒ è°ƒç”¨ Azure OpenAI å¤±è´¥ï¼š")
    print(f"   {type(e).__name__}: {e}")
    print("\nè¯·æ£€æŸ¥ï¼š")
    print("  1. endpoint æ˜¯å¦æ­£ç¡®ï¼ˆhttps://<èµ„æºå>.openai.azure.com/ï¼‰")
    print("  2. api_key æ˜¯å¦å¯¹åº”è¯¥èµ„æº")
    print("  3. deployment åç§°æ˜¯å¦å’Œ Azure é—¨æˆ·ä¸­çš„éƒ¨ç½²åç§°ä¸€è‡´")
    print("  4. api_version æ˜¯å¦ä¸ºè¯¥éƒ¨ç½²æ”¯æŒçš„ç‰ˆæœ¬")
    print("  5. å…¬å¸ç½‘ç»œ / ä»£ç†æ˜¯å¦å…è®¸è®¿é—® *.openai.azure.com")
    raise
